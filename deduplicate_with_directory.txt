#!/usr/bin/env python
import os
import hashlib
import csv
from collections import defaultdict


# Data structures for file hashes and directory contents
hashes_full = {}
dir_contents = defaultdict(list)  # Maps directories to their file hashes

# Log duplicate directories
csvfile = open('duplicate_directories.csv', 'w', newline='')
csvwriter = csv.writer(csvfile)
csvwriter.writerow(['Directory1', 'Directory2', 'Size (MB)'])


def chunk_reader(fobj, chunk_size=1024):
    """Generator that reads a file in chunks of bytes."""
    while True:
        chunk = fobj.read(chunk_size)
        if not chunk:
            return
        yield chunk


def get_file_hash(filepath, hash_algo=hashlib.sha1):
    """Calculate a hash for the given file."""
    hash_obj = hash_algo()
    try:
        with open(filepath, 'rb') as file_obj:
            for chunk in chunk_reader(file_obj):
                hash_obj.update(chunk)
    except OSError:
        return None
    return hash_obj.hexdigest()


def index_files_and_directories(paths):
    """
    Index all files and directories, computing file hashes and associating them
    with their respective directories.
    """
    print("Indexing files and directories...")
    for path in paths:
        for dirpath, _, filenames in os.walk(path):
            print(f"\tProcessing: {os.path.abspath(dirpath)}")
            for filename in filenames:
                full_path = os.path.join(dirpath, filename)
                try:
                    full_path = os.path.realpath(full_path)
                    file_hash = get_file_hash(full_path)
                    if file_hash:
                        hashes_full[file_hash] = full_path
                        dir_contents[dirpath].append(file_hash)
                except OSError:
                    continue


def are_directories_duplicates(dir1, dir2):
    """
    Check if two directories are duplicates by comparing their file hashes.
    """
    return set(dir_contents[dir1]) == set(dir_contents[dir2])


def find_duplicate_directories():
    """
    Identify duplicate directories and recursively escalate to higher-level duplicates.
    """
    print("Identifying duplicate directories...")
    checked_pairs = set()
    total_dupe_size_mb = 0

    # Track directories grouped by their file hashes
    dir_hash_map = defaultdict(list)
    for dirpath, file_hashes in dir_contents.items():
        dir_hash_map[frozenset(file_hashes)].append(dirpath)

    # Compare directories with identical contents
    for dir_group, dirs in dir_hash_map.items():
        if len(dirs) > 1:
            for i, dir1 in enumerate(dirs[:-1]):
                for dir2 in dirs[i + 1:]:
                    if (dir1, dir2) in checked_pairs or (dir2, dir1) in checked_pairs:
                        continue
                    checked_pairs.add((dir1, dir2))

                    # Escalate to parent directories
                    while True:
                        if are_directories_duplicates(dir1, dir2):
                            size_mb = sum(os.path.getsize(os.path.join(root, f))
                                          for root, _, files in os.walk(dir1)
                                          for f in files) / (1024 * 1024)
                            total_dupe_size_mb += size_mb
                            csvwriter.writerow([dir1, dir2, "%.02f" % size_mb])
                            print(f"Duplicate directories: {dir1} and {dir2} ({size_mb:.2f} MB)")

                            # Move to parent directories
                            new_dir1 = os.path.dirname(dir1)
                            new_dir2 = os.path.dirname(dir2)
                            if new_dir1 == dir1 or new_dir2 == dir2:
                                break  # Reached the root
                            dir1, dir2 = new_dir1, new_dir2
                        else:
                            break

    if total_dupe_size_mb == 0:
        print("No duplicate directories found.")
    else:
        print(f"Total duplicate size: {total_dupe_size_mb:.2f} MB")


if __name__ == "__main__":
    import sys

    if sys.argv[1:]:
        index_files_and_directories(sys.argv[1:])
        find_duplicate_directories()
    else:
        print("Please pass the paths to check as parameters to the script.")

    csvfile.close()
